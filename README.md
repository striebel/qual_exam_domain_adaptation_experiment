# Domain adaptation experiment

Instructions for running the experiment on Indiana University's
Carbonate cluster.

## Contents

* <a href='#environment'>Environment</a>
* <a href='#data'>Data</a>
* <a href='#train-and-predict'>Train and predict</a>
* <a href='#evaluate'>Evaluate</a>
* <a href='#visualize-results'>Visualize results</a>

<h2 id='environment'>Environment</h2>

### Carbonate access

To enable ssh key access, first see the
[system access section of the general knolwedge base
article about Carbonate](https://kb.iu.edu/d/aolp#access).
Read that section, then follow the link to the
[SSH Agreement](https://hpceverywhere.iu.edu/forms/agree)
and sign the agreement.

Generate a new ssh key pair with 
```sh
$ ssh-keygen
```
Press enter to accept the default location, or type
the path to a preferred location to store the new
key pair, for example,
```sh
/home/local_username/.ssh/id_rsa_carbonate
```
Then enter a key passphrase that satisfies the
SSH Agreement.

Copy the key to the remote host, for example, using
```sh
$ ssh-copy-id \
    -i /home/local_username/.ssh/id_rsa_carbonate.pub \
    iu_username@carbonate.uits.iu.edu
```

Confirm that they key was transfered by logging in to Carbonate:
one should only have to enter the ssh key's passphrase;
one should not be prompted to use a Duo second factor
method.

If prompted with a dialog to enter the ssh key passphrase
and if the dialog has a checkbox to remember the passphrase,
select the checkbox.
But if a dialog does not appear or if the dialog does not have
a checkbox, then see
[here](https://superuser.com/a/990447) and
[here](https://unix.stackexchange.com/a/571744)
for how to configure the ssh key passphrase to
be loaded automatically when logging in to the local machine
and to be applied automatically when
ssh'ing into Carbonate.

### Obtain and set up compute node

ssh into a Carbonate login node with
```sh
ssh <username>@carbonate.uits.iu.edu
```

For use with [Slurm](https://en.wikipedia.org/wiki/Slurm_Workload_Manager),
add the line
```sh
export ACCOUNT=<account_id>
```
to your `~/.bashrc` file, then run
```sh
. ~/.bashrc
```

Next request that a compute node be allocated to you
(this command can be saved in a shell script):
```sh
srun \
    --account=$ACCOUNT \
    --partition=gpu \
    --gres=gpu:1 \
    --nodes=1 \
    --time=12:00:00 \
    --pty bash
```

Next, via
[Environment Modules](https://en.wikipedia.org/wiki/Environment_Modules_(software)),
load a recent version of Python 3:
```sh
module load python/3.9.8
```

Clone the experiment repository:
```sh
git clone git@github.com:striebel/domain_adaptation_experiment.git
```

cd into the clone repo and
open `set_vars.sh` in a text editor and update the variables
as desired. Then run
```sh
. set_vars.sh
```

### Create venv and install parser

followed by
```sh
scripts/setup/venv.sh
```
to create a Python virtual environment into which the parser will be installed.

The script
```sh
scripts/setup/parser/download.sh
```
will download a fresh version of the parser, but this script should not be run,
because a modified version of the parser is already included in this repo.

However, the following parser installation script should be run:
```sh
scripts/setup/parser/install.sh
```

<h2 id='data'>Data</h2>

First download the treebanks (UD EWT and GUM):
```sh
scripts/setup/data/download.bash
```

Convert the treebanks from CONLLU format to CONLL, simplifying them, for example,
by removing multiword token summary lines:
```sh
scripts/setup/data/simplify.sh
```

Generate a mapping from each sentence in EWT and GUM to its source domain
by inspecting each sentence's metadata:
```sh
scripts/setup/data/annotate_domain.sh
```

Print domain statistics (the number of domains in EWT and in GUM and the
number of sentences in each domain) by running the following script, but this script
doesn't do anything other than print the stats to the terminal:
```sh
scripts/setup/data/stats.sh
```

This experiment uses 10-fold cross-validation and the folds that were previously used
are included in the `<repo_root>/folds` dir.
However, folds can be regenerated by running the following script.

Execute
```sh
scripts/setup/data/make_folds.sh
```
to randomly select the data that will be used in the experiments.
1,000 sentences are selected from each of the five domains in EWT.
GUM has 11 domains, but only five of them have more than 1,000
sentences, so these five domains are used and 1,000 sentences are
randomly selected from each.
The result is ten domains with 1,000 sentences each, 10,000 sentences
total, are available to create train, dev, and test partitions.
The script `make_folds.sh` will do nothing if the dir
`$REPO_DIR/folds` already exists, so if you would like to randomly
select a new set of sentences to use in the experiments,
`rm -rf $REPO_DIR/folds` should be run first.
(The name `make_folds` may be kind of misleading, because
the script only makes the folds indirectly; that is,
it samples 10,000 sentences as described above and gives them
a specific order with the partition points for the folds later
used between indices being:
999 and 1000; 1999 and 2000; ...; 8999 and 9000; 9999 and 0.)

Then actually generate the ten folds of train, dev, and test conllu files with
the following:
```sh
scripts/setup/data/generate_conllu_files.sh
```

<h2 id='train-and-predict'>Train and predict</h2>

Generate all the config files and scripts needed to both train the models and
run predict on the test data:
```sh
scripts/setup/generate_scripts.sh
```

Execute
```sh
scripts/train/dispatch_all.sh
```
in order to submit all of the jobs to Slurm for training and running predict
on the test data.

<h2 id='evaluate'>Evaluate</h2>

Execute
```sh
scripts/eval/my_eval_c.sh
```
followed by
```sh
scripts/eval/my_eval_d.sh
```

Then tar and compress the `<repo_root>/results` dir and copy it to your local
machine.

<h2 id='visualize-results'>Visualize results</h2>

Install Jupyter Lab locally, and open `scripts/visualize/visualize.ipynb`.
Run this notebook to generate charts that show the results.

from allennlp.data.tokenizers import PretrainedTransformerTokenizer


tokenizer1 = PretrainedTransformerTokenizer('bert-base-multilingual-cased')
tokenizer2 = PretrainedTransformerTokenizer('xlm-mlm-tlm-xnli15-1024')


data1 = [   101,  13646,  10124,  12976,  10124,  10978,  10114,  84630,  10169,
          31617,  14829,  13518,  10340,    117,  10106,  15127,  32282,    117,
          12373,  10261,  10393,  10464,  11264,  42153,  58768,  26426,    118,
            169,  12558,  32342,  59893,  28504,  10106,  10226,  56294,    118,
          10319,  20562,  10957,  27185,  10153,  48798,  23122,  11203,  10142,
          10105,  12956,  10108,  22872,  14100,  17086,    131,  10357,  15107,
          10472,  30181,  68312,  10169,  10105,  11787,  10108,    107,  50177,
          10107,  10111, 100777,  10107,    107,  12183,  10708,  10105,  24160,
            117,  10111,  10261,  10124,  42374,  10114,  15148,  57675,  13135,
          10271,  12935,  10106,  12990,  10114,  28086,  10105,    107,  19919,
          14926,  23612,  21271,    117,    107,    102,    101,  10319,  31886,
          10142,  35472,  10454,    118,  19299,  55505,  12952,  10160,  10105,
          11419,  34711,  10108,  15984,  10111,  10105,  22872,  14100,    119,
            102]
data2 = [    0,  6795,   282, 27113,   282, 37103,   322, 83433, 10602, 65570,
         12950,   128,  1930,    14,    28,  2752, 17001,    14, 67103,  1456,
         27070,  1191,  2146,  1760,   228, 16096,    38,    23, 34868, 77142,
         46565, 53930,    28, 19663, 75289,    38, 52334, 70922, 36234, 87537,
           234, 13729,  8176,  3269,   147,  2798,   294, 26711,  4876, 10537,
            42,  1456, 67601, 10829, 70799, 75875, 10602,   147,  2702,   294,
            17, 22305,  3799,   530,   723, 10145,    17, 77289,  7836,   147,
         12437,    14,   530,  1456,   282, 57811,   322, 12228,   389, 64478,
           458, 14834,    28, 31840,   322, 71776,   147,    17, 34824,  7271,
          8295,  3032, 44405,    14,    17,     1,     0, 52334, 82111,  3269,
         32279,  1619,    38, 85929,  2869, 42486,    82, 16718,   477,   147,
           498, 22126,   294, 47633,   530,   147, 26711,  4876,    15,     1]

data2 = [2,0]
def unTok(tokenizer, data):
    tokenizer = PretrainedTransformerTokenizer(tokenizer)
    print(len(data))
    for wordpieceIdx in data:
        print(tokenizer.tokenizer._convert_id_to_token(wordpieceIdx), end= ' ')
    print()

unTok('bert-base-multilingual-cased', data1)
unTok('bert-base-multilingual-cased', data2)
unTok('xlm-mlm-tlm-xnli15-1024', data2)




